<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>body {
  max-width: 980px;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable,
.markdown-body .highlighttable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr,
.markdown-body .highlighttable {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite,
.markdown-body .highlighttable pre,
.markdown-body .highlighttable div.highlight {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td,
.markdown-body .highlighttable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  border-radius: 3px;
}

.markdown-body code:not(.highlight):not(.codehilite), .markdown-body samp {
  background-color: rgba(0,0,0,0.04);
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite,
.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
}

.markdown-body .codehilite,
.markdown-body .highlight,
.markdown-body pre {
  border-radius: 3px;
}

.markdown-body :not(.highlight) > pre {
  background-color: #f7f7f7;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

.markdown-body diagram-div, .markdown-body div.uml-sequence-diagram, .markdown-body, div.uml-flowchart {
  overflow: auto;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*GitHub*/
.highlight {background-color:#f7f7f7;color:#333333;}
.highlight .hll {background-color:#ffffcc;}
.highlight .c{color:#999988;font-style:italic}
.highlight .err{color:#a61717;background-color:#e3d2d2}
.highlight .k{font-weight:bold}
.highlight .o{font-weight:bold}
.highlight .cm{color:#999988;font-style:italic}
.highlight .cp{color:#999999;font-weight:bold}
.highlight .c1{color:#999988;font-style:italic}
.highlight .cs{color:#999999;font-weight:bold;font-style:italic}
.highlight .gd{color:#000000;background-color:#ffdddd}
.highlight .ge{font-style:italic}
.highlight .gr{color:#aa0000}
.highlight .gh{color:#999999}
.highlight .gi{color:#000000;background-color:#ddffdd}
.highlight .go{color:#888888}
.highlight .gp{color:#555555}
.highlight .gs{font-weight:bold}
.highlight .gu{color:#800080;font-weight:bold}
.highlight .gt{color:#aa0000}
.highlight .kc{font-weight:bold}
.highlight .kd{font-weight:bold}
.highlight .kn{font-weight:bold}
.highlight .kp{font-weight:bold}
.highlight .kr{font-weight:bold}
.highlight .kt{color:#445588;font-weight:bold}
.highlight .m{color:#009999}
.highlight .s{color:#dd1144}
.highlight .n{color:#333333}
.highlight .na{color:teal}
.highlight .nb{color:#0086b3}
.highlight .nc{color:#445588;font-weight:bold}
.highlight .no{color:teal}
.highlight .ni{color:purple}
.highlight .ne{color:#990000;font-weight:bold}
.highlight .nf{color:#990000;font-weight:bold}
.highlight .nn{color:#555555}
.highlight .nt{color:navy}
.highlight .nv{color:teal}
.highlight .ow{font-weight:bold}
.highlight .w{color:#bbbbbb}
.highlight .mf{color:#009999}
.highlight .mh{color:#009999}
.highlight .mi{color:#009999}
.highlight .mo{color:#009999}
.highlight .sb{color:#dd1144}
.highlight .sc{color:#dd1144}
.highlight .sd{color:#dd1144}
.highlight .s2{color:#dd1144}
.highlight .se{color:#dd1144}
.highlight .sh{color:#dd1144}
.highlight .si{color:#dd1144}
.highlight .sx{color:#dd1144}
.highlight .sr{color:#009926}
.highlight .s1{color:#dd1144}
.highlight .ss{color:#990073}
.highlight .bp{color:#999999}
.highlight .vc{color:teal}
.highlight .vg{color:teal}
.highlight .vi{color:teal}
.highlight .il{color:#009999}
.highlight .gc{color:#999;background-color:#EAF2F5}
</style><title>building_llms_from_scratch</title></head><body><article class="markdown-body"><h1 id="building-llms-from-scratch">Building LLMs from scratch<a class="headerlink" href="#building-llms-from-scratch" title="Permanent link"></a></h1>
<h2 id="chapter-1-what-is-llm">Chapter 1 - What is LLM<a class="headerlink" href="#chapter-1-what-is-llm" title="Permanent link"></a></h2>
<ul>
<li>LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.</li>
<li>few-shot capabilities means a model can learn to perform new tasks based on only a few examples instead of needing extensive training data.</li>
<li>there are two stages of training:<ul>
<li>pretraining: here the model is trained over large unlabelled text. this model is called the foundation model and can be used to complete a sentence for example</li>
<li>fine tuning: here the model is trained on a labelled data. Fine tuning is further divided into two types:<ul>
<li>instruction fine tuning: e.g. providing instructions and answers (e.g. a translation from one language to the other)</li>
<li>classification fine tuning: e.g. email classification like spam, promotion, etc.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="transformer-architecture">Transformer architecture<a class="headerlink" href="#transformer-architecture" title="Permanent link"></a></h3>
<p><img alt="Transformer architecture" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/transformer_architecture.png" /></p>
<ul>
<li>Encoder embeds, i.e. encodes the input in a numerical form (e.g. one hot encoding)</li>
<li>Decoder consumes this encoded vector to produce the output. The figure here is showing that the decoder has already translated the sentence up to the last word and then all the previously translated words are being used to spit out the last word &ldquo;Biespiel&rdquo;</li>
<li>A key component of transformers and LLMs is the self-attention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.</li>
</ul>
<h3 id="gpt-vs-bert">GPT vs BERT<a class="headerlink" href="#gpt-vs-bert" title="Permanent link"></a></h3>
<ul>
<li>BERT (bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks</li>
<li>GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction</li>
</ul>
<p><img alt="BERT architecture" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/bert_architecture.png" /></p>
<h4 id="n-shot-learning">n-shot learning<a class="headerlink" href="#n-shot-learning" title="Permanent link"></a></h4>
<p><img alt="n-shot learning" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/n_shot_learning.png" /></p>
<h3 id="gpt-architecture">GPT architecture<a class="headerlink" href="#gpt-architecture" title="Permanent link"></a></h3>
<ul>
<li>GPT architecture is simplified version of the transformer architecture in the sense that it has only the decoder part. But it also is much bigger. e.g.  original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. </li>
<li>GPT is remarkable because it essentially runs unsupervised on unlabelled data because it uses it&rsquo;s own generated prvious data to predict the next data</li>
</ul>
<p><img alt="GPT architecture " src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/gpt_architecture.png" /></p>
<h3 id="training-an-llm">Training an LLM<a class="headerlink" href="#training-an-llm" title="Permanent link"></a></h3>
<p><img alt="Training an llm" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/llm_plan.png" /></p>
<h2 id="chapter-2-working-with-text-data">Chapter 2 - Working with text data<a class="headerlink" href="#chapter-2-working-with-text-data" title="Permanent link"></a></h2>
<h3 id="embedding">Embedding<a class="headerlink" href="#embedding" title="Permanent link"></a></h3>
<ul>
<li>The concept of converting data into a vector format is often referred to as embedding. Embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space</li>
<li>different data types (text, audio, video) require different embeddings. Infact, even for text, there are different types of embedding. e.g. word, sentence, or paragraph level</li>
<li>embedding is also done with the help of an embedding model (neural network)</li>
<li>Word2Vec is one popular example of embedding model.  With Word2Vec words that appear in similar contexts tend to have similar meanings. Consequently, when projected into two-dimensional word embeddings for visualization purposes, similar terms are clustered together (e.g. eagle, duck, goose will be in one group, car, bike, truck will be clustered in another group, etc.)</li>
<li>Word embeddings can have varying dimensions ranging from 1 to thousands</li>
</ul>
<h4 id="tokenization">Tokenization<a class="headerlink" href="#tokenization" title="Permanent link"></a></h4>
<h5 id="basic-encoding">Basic encoding<a class="headerlink" href="#basic-encoding" title="Permanent link"></a></h5>
<ul>
<li>
<p>Steps:</p>
<ol>
<li>parse the text and do the cleanup as required (e.g. remove white spaces, separate out punctuations from the words. (e.g. &ldquo;Is this a question? &ndash; asked Renee&rdquo; &ndash;&gt; [&lsquo;is&rsquo;, &lsquo;this&rsquo;, &lsquo;a&rsquo;, &lsquo;question&rsquo;, &lsquo;?&rsquo;. &ndash;&lsquo;, &lsquo;asked&rsquo;, &lsquo;Renee&rsquo;])</li>
<li>Convert the tokens to integers (token-ids). This is done by using a lookup which maps words to a corresponding integer value. Let&rsquo;s say we wanted to assign token ids to all words in a book. We&rsquo;d parse all words, numbers, punctuations, etc. from a book and arrange them in alphabetical order while removing duplicates. Then we&rsquo;d assign integer ids to these words in the alphabetical order. This is our dictionary. Then for any given sentence, we just lookup this dictionary.</li>
<li>Enhance the tokenizer to account for unknown words and mark boundaries between separate things (e.g. paragraphs, different documents, etc.) See below:</li>
</ol>
<p><img alt="Tokenization Enhancement" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/tokenization_with_context.png" /></p>
<p>There are other variants of these special meaning tokens, like BOS, EOS (beginning and end of sequence), PAD (if it&rsquo;s required that all text is the same size when training in batches, shorted text can be padded with paddning tokens)</p>
</li>
</ul>
<h5 id="byte-pair-encoding-bpe">Byte pair encoding (BPE)<a class="headerlink" href="#byte-pair-encoding-bpe" title="Permanent link"></a></h5>
<ul>
<li>Byte pair encoding allows for not only encoding unknown words but also retrieving them as-is. As opposed to the previous approach where the tokenizer always encoded the unknown words with the same tag. So, when decoding, there is no way to differential between different unknown words</li>
<li>BPE does this by breaking down the unknown words into known words, as much as possible or even at the letter level if possible. e.g. if the word to be encoded was &lsquo;someUknownWord&rsquo; it will be broken down into &lsquo;some&rsquo;, &lsquo;unknown&rsquo; and &lsquo;word&rsquo;, all three of which are well known. One example is shown below:</li>
</ul>
<p><img alt="BPE Tokenization of Unknown Words" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/bpe_tokenization_algo.png" /></p>
<h4 id="data-sampling-with-a-sliding-window">Data sampling with a sliding window<a class="headerlink" href="#data-sampling-with-a-sliding-window" title="Permanent link"></a></h4>
<ul>
<li>The next step after encoding the tokens in the numerical values is to generate input target pairs</li>
<li>In this step, we need to prepare the input data and the expected output that LLM is supposed to produce. This is where the sliding window approach comes into picture. See example below:</li>
</ul>
<p><img alt="LLM Training Input" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/llm_training_input.png" /></p>
<ul>
<li>Assuming we select the context size = 4, i.e. the MAX number of tokens included in the input, sample training input (<code>x</code>) and the target (<code>y</code>) will look like so:</li>
</ul>
<div class="highlight"><pre>x: [290, 4920, 2241, 287]
y:      [4920, 2241, 287, 257]
</pre></div>

<p>Here, <code>y</code> is <code>x</code> shifted by 1. And we use it for training as follows:
- we provide x values up to <code>n-1</code> and provide the target at <code>n</code>. When we put this in a loop, training data will look like this:</p>
<div class="highlight"><pre>x ----&gt; y       
[290] ----&gt; 4920
[290, 4920] ----&gt; 2241
[290, 4920, 2241] ----&gt; 287
[290, 4920, 2241, 287] ----&gt; 257
</pre></div>

<p>In real world, this could look something like this:</p>
<div class="highlight"><pre>and ----&gt;  established
 and established ----&gt;  himself
 and established himself ----&gt;  in
 and established himself in ----&gt;  a
</pre></div>

<p>Notice that the number of input tokens increment up to the predetermined context size (4) and the next expected <code>y</code> is provided. That means, it is important to keep at least some context in order to predict the correct <code>y</code> so the sentence can be coherent. A significant amount of tuning effort is probably required to determine an optimal context size.</p>
<h4 id="conversion-to-tensor-datasets-and-generating-batches">Conversion to tensor datasets and generating batches<a class="headerlink" href="#conversion-to-tensor-datasets-and-generating-batches" title="Permanent link"></a></h4>
<ul>
<li>Once the training and target tokens are prepared, they can be represented as pytorch tensors like this:</li>
</ul>
<p><img alt="Tokens to tensors" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/tokens_to_tensors.png" /></p>
<p>Notice here that the length of the row is 4 because that&rsquo;s the context window size we selected. In reality, this size is at least 256.
Also, the words are shown in the image instead of the actual tokens for clarity.</p>
<ul>
<li>The pytorch tensors are actually underlying datatypes for the pythorch datasets, which are the <code>Dataset</code> type classes. The whole purpose of these classes is to provide efficient iteration (by implementing dunder methods). Example code:</li>
</ul>
<p>```python
import torch
from torch.utils.data import Dataset, DataLoader
class GPTDatasetV1(Dataset):
    def <strong>init</strong>(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []</p>
<pre><code>    token_ids = tokenizer.encode(txt)    #1

    for i in range(0, len(token_ids) - max_length, stride):     #2
        input_chunk = token_ids[i:i + max_length]
        target_chunk = token_ids[i + 1: i + max_length + 1]
        self.input_ids.append(torch.tensor(input_chunk))
        self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):    #3
    return len(self.input_ids)

def __getitem__(self, idx):         #4
    return self.input_ids[idx], self.target_ids[idx]
</code></pre>
<p>```</p>
<ul>
<li>The <code>stride</code> param has to do with batching. Batches are the chunks of data with which the networks are trained. </li>
</ul>
<p><img alt="Batching" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/batching.png" /></p>
<h2 id="chapter-3-coding-attention-mechanism">Chapter 3 Coding Attention Mechanism<a class="headerlink" href="#chapter-3-coding-attention-mechanism" title="Permanent link"></a></h2>
<p>Three main stages of coding an LLM</p>
<p><img alt="Three Stages of Coding an LLM" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/llm_building_stages.png" /></p>
<p>Before the advent of the transformer models, the RNNs (recurrent neural networks) were a popular choice for tasks like translation. Without going into much details, it suffices to know that in RNNs, the encoder part processes the entire input text into a hidden state. The decoder then takes in this hidden state to produce the output. 
Think of this as a deep neural network divided into input and output layers. When the output layer starts doing its thing, it does so, only based on the connections with the layer it is connected to. That is, it does not have access to the earlier layers.
Now imagine that a long sentence needs to be translated and your DNN can only accept 5 words as inputs at a time. That means, the last layer of the the encoding part will only have the context worth 5 words and therefore the decoder will also have the context worth 5 words only - which may not be enough for the decoder to generate the accurate next word in the translation.
The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.</p>
<p>To solve this problem for RNN, Bahdanau attention mechanism  was proposed that allowed the decoder layer to have access to all input tokens. The &ldquo;attention&rdquo; part here is that, the input tokens are also assigned weights by the encoder layers according to their importance.</p>
<p>Although, very soon after the Bahdanau attention mechanism was introduced, reserchers figured out that the RNNs are not necessary for natural language processing at all. And proposed the original transformer architecture discussed in chapter 1.</p>
<h3 id="self-attention">Self attention<a class="headerlink" href="#self-attention" title="Permanent link"></a></h3>
<p>Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or “attend to”, all other positions in the same sequence when computing the representation of a sequence. Let&rsquo;s break this down:
- when computing the representation of a sequence: That means the goal is to compute representation of a sequence. This sequence is probably the embedding vector
- mechanism that allows each position in the input sequence to consider the relevancy of, or “attend to”, all other positions in the same sequence: means we have an input sequence and we want to consider the relevancy to all other positions when we calculate the representation of a given position. e.g. in the sentence Roses are red, if we were to represent &lsquo;red&rsquo;, we could just represent it as a single word or we could represent it by also considering the sequence &ldquo;roses are&rdquo; that follows before the word &ldquo;red&rdquo;</p>
<p><strong>Self-attention</strong> vs <strong>attention</strong></p>
<ul>
<li>self refers to the attention within the same sequence (e.g. pixels within a picture, words in a given sentence). The mechanism computes the &ldquo;attention weights&rdquo; by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.</li>
<li>in traditional attention mechanisms, the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence</li>
</ul>
<h3 id="simple-self-attention-mechanism-without-trainable-weights">Simple self-attention mechanism without trainable weights<a class="headerlink" href="#simple-self-attention-mechanism-without-trainable-weights" title="Permanent link"></a></h3>
<p><img alt="Self Attention Context vector" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/self_attention_context_vector.png" /></p>
<p>The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. In this example, we compute the context vector z(2). The importance or contribution of each input element for computing z(2) is determined by the attention weights a21 to a2T. When computing z(2), the attention weights are calculated with respect to input element x(2) and all other inputs.</p>
<p>To illustrate this concept, let’s focus on the embedding vector of the second input element, x(2) (which corresponds to the token &ldquo;journey&rdquo;), and the corresponding context vector, z(2), shown at the bottom of figure 3.7. This enhanced context vector, z(2), is an embedding that contains information about x(2) and all other input elements, x(1) to x(T).</p>
<p><img alt="Attention score example" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/attention_score_example.png" /></p>
<p>In the screenshot above, we are using the word &ldquo;journey&rdquo; as a reference token. To calculate the attention score for all other tokens in the sentence, we simply take a dot product of the embedding vector for &ldquo;journey&rdquo; with embedding vectors of other words. 
e.g. the dot product of <code>journey</code> with <code>Your</code> would be <code>[0.55,0.87,0.66] dot [0.43,0.15,0.89] ~= 0.9</code>. Note that these weights are shown with less precision in the diagram to save space, therefore the descrepency. Actual numbers are:
<div class="highlight"><pre>inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)
</pre></div></p>
<p><strong>Why dot product</strong>
- dot product is a measure of similarity because it quantifies how closely two vectors are aligned. (remember, dot product is the sum of a products of elements at the same index. Therefore, the dot product will be higher if the same index elements are higher)</p>
<p>For the concerned input &ldquo;journey&rdquo;, the attention scores look like this:</p>
<div class="highlight"><pre>tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
</pre></div>

<p><strong>Normalize attention weights</strong></p>
<p>We then normalize the attention weights:</p>
<p><img alt="Normalized attention weights" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/normalized_attention_weights.png" /></p>
<p>In practise, it&rsquo;s better to use the softmax function for normaization to achieve managing extreme values and more favorable gradient properties during training. Even naive softmax may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it’s advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance.</p>
<p>For the concerned input  &ldquo;journey&rdquo; the normalized attention weights look like this:</p>
<div class="highlight"><pre>Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)
</pre></div>

<p><strong>Calculate Context Vector</strong></p>
<p>Finally, the context vector is calculated as follows:</p>
<p><img alt="Context vector calculation" src="/C:/Users/sdevikar/OneDrive%20-%20Qualcomm/Documents/personal/training/building_llm_from_scratch/images/context_vector_calc.png" /></p>
<p>Here, we&rsquo;re multiplying the input cells with attention weights and finally adding each column to the final context vector. So considering the inputs tensor, the math would look like this:
<div class="highlight"><pre>inputs = torch.tensor(
  [[0.43, 0.15, 0.89], * 0.1    = [0.043, 0.015, 0.089]
   [0.55, 0.87, 0.66], * 0.2    = [0.11, 0.174, 0.132]
   [0.57, 0.85, 0.64], * 0.2    = [0.114, 0.170, 0.128]
   [0.22, 0.58, 0.33], * ...    = [...]
   [0.77, 0.25, 0.10], * ...    = [...]
   [0.05, 0.80, 0.55]] * 0.1    = [0.005, 0.080, 0.055]
)
total                           = [0.4419, 0.6515, 0.5683]
</pre></div></p>
<p>Note once again that this is the context vector for input1 i.e. the word &ldquo;journey&rdquo;. When we calculate this for all the words in the context window, we will get a context vector for each word like so:</p>
<div class="highlight"><pre>tensor([[0.4421, 0.5931, 0.5790], # context vector for &quot;your&quot;
        [0.4419, 0.6515, 0.5683], # context vector for &quot;journey&quot;
        [0.4431, 0.6496, 0.5671], # context vector for &quot;starts&quot;
        [0.4304, 0.6298, 0.5510], # context vector for &quot;with&quot;
        [0.4671, 0.5910, 0.5266], # context vector for &quot;one&quot;
        [0.4177, 0.6503, 0.5645]]) # context vector for &quot;step&quot;
</pre></div>

<p><strong>Intuition summary of what we did above</strong></p>
<ol>
<li>We started with the embedding vectors for all the words in our context window. Keep in mind that these embedding vectors are not random numbers but representation of the words themselves. Therefore, &ldquo;similar&rdquo; words (by some measure), will have similar looking vectors. e.g. <code>journey = [0.55, 0.87, 0.66] and starts = [0.57, 0.85, 0.64]</code> have very similar embedding vectors because they are probably seen in the training set together very often</li>
<li>Then we used these embedding vectors to calculate a &ldquo;similarity coefficient&rdquo; by calculating the dot product of embedding vectors - we call these &ldquo;attention scores&rdquo;. Again, if you look at the normalized attention scores (<code>([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])</code> for &ldquo;journey&rdquo;), you can see that the attention score from journey-&gt;journey and journey-&gt;starts is almost identical (0.2379 vs 0.2333)</li>
<li>Now that we have calculated how much relevance a given word is to other words close to it, we use that information to calculate a weigthed sum. This sum is like a modification of the word&rsquo;s vector embedding. The modification accounts for the word&rsquo;s context. e.g. the word journey had an embedding vector = <code>[0.55, 0.87, 0.66]</code> before accounting for the context. Now we accounted for the context and the new embedding vector is <code>[0.4419, 0.6515, 0.5683]</code>. Think about this for a second - if a word in question is surrounded by very dissimilar words, the attention weights assigned to them would be small and as a result, the context vector would have a smaller magnitude. Therefore, it becomes an unimportant word. Conversely, if a word is surrounded by similar words, the magnitude of the context vector would be large and therefore the word becomes important.</li>
</ol>
<h3 id="implementing-self-attention-with-trainable-weights">Implementing self-attention with trainable weights<a class="headerlink" href="#implementing-self-attention-with-trainable-weights" title="Permanent link"></a></h3>
<p>In the last step above, we calculated the <em>simplified</em> context vector by simply adding the attention weights. The actual self attention mechanism takes this a step further and calculates the self attention weights as the weigthed sum over the input vectors specific to a given input vector. </p></article></body></html>